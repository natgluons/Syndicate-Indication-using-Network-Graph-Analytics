{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natgluons/GNNs_HGTmodel/blob/main/HGTmodel_GNNs_fraud_nonfraud_paper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruN4XUdiVGPT",
        "outputId": "8711f9d9-666e-4a7b-9df1-08a204bb5883"
      },
      "outputs": [],
      "source": [
        "install_deps = False\n",
        "if install_deps:\n",
        "    # %pip install https://download.pytorch.org/whl/cu121/torch-2.3.0%2Bcu121-cp312-cp312-win_amd64.whl\n",
        "    # %pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cpu\n",
        "    %pip install torch==2.2.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
        "    %pip install cudatoolkit\n",
        "    # %pip install torch_geometric\n",
        "    %pip install kneed\n",
        "    %pip install google-cloud-bigquery\n",
        "    %pip install db-dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "# import warnings\n",
        "# import pandas_gbq\n",
        "# from sklearn.exceptions import ConvergenceWarning\n",
        "# from torch_geometric.utils import to_networkx\n",
        "# from torch_geometric.data import Data\n",
        "from torch_geometric.nn import RGCNConv\n",
        "# from torch.optim import Adam\n",
        "# from torch.nn.functional import log_softmax, nll_loss\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.cluster import KMeans\n",
        "# from kneed import KneeLocator\n",
        "from sklearn.manifold import TSNE\n",
        "# from sklearn.metrics.pairwise import euclidean_distances\n",
        "# from collections import defaultdict\n",
        "# from tabulate import tabulate\n",
        "from google.cloud import bigquery\n",
        "# from google.oauth2 import service_account"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "runtime = \"local\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFHyXMfq4HKD",
        "outputId": "44585c18-054d-4d8f-db1f-ab37971108a5"
      },
      "outputs": [],
      "source": [
        "if runtime == \"colab\":\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xfeXcaRnEkiJ"
      },
      "outputs": [],
      "source": [
        "def query(project_id):\n",
        "  query = \"\"\"\n",
        "  WITH RECURSIVE\n",
        "MerchantData AS (\n",
        "  SELECT\n",
        "    CAST(biz_org_id AS STRING) AS user_id,\n",
        "    'merchant' AS type,\n",
        "    province,\n",
        "    business_type_desc AS type_desc,\n",
        "    business_segment,\n",
        "    business_sub_segment,\n",
        "    TIMESTAMP_DIFF(CURRENT_DATE(), created_datetime, MONTH) AS account_age_months\n",
        "  FROM `project-linkaja-dataset.pii.dim_merchant`\n",
        "),\n",
        "\n",
        "CustomerData AS (\n",
        "  SELECT\n",
        "    CAST(customer_id AS STRING) AS user_id,\n",
        "    'customer' AS type,\n",
        "    province,\n",
        "    trust_level_desc AS type_desc,\n",
        "    '-' AS business_segment,\n",
        "    '-' AS business_sub_segment,\n",
        "    TIMESTAMP_DIFF(CURRENT_DATE(), created_time, MONTH) AS account_age_months\n",
        "  FROM `project-linkaja-dataset.pii.dim_customer`\n",
        "),\n",
        "\n",
        "MerchantHitCounts AS (\n",
        "  SELECT\n",
        "    CAST(merchant_id AS STRING) AS user_id,\n",
        "    COUNT(*) AS hit_count\n",
        "  FROM `project-self-service-query.fraud_hotlist.dm_fds_rule_hit`\n",
        "  GROUP BY merchant_id\n",
        "),\n",
        "\n",
        "CustomerHitCounts AS (\n",
        "  SELECT\n",
        "    CAST(customer_id AS STRING) AS user_id,\n",
        "    COUNT(*) AS hit_count\n",
        "  FROM `project-self-service-query.fraud_hotlist.dm_fds_rule_hit`\n",
        "  GROUP BY customer_id\n",
        "),\n",
        "\n",
        "MerchantRiskStatus AS (\n",
        "  SELECT\n",
        "    CAST(biz_org_id AS STRING) AS user_id,\n",
        "    SUM(\n",
        "      CASE WHEN status_desc = 'blocked' OR status_desc = 'frozen'\n",
        "      OR biz_org_id IN (SELECT user_id FROM `project-self-service-query.fraud_hotlist.reported_merchant_customer`) THEN 2\n",
        "      WHEN status_desc = 'suspended' THEN 1\n",
        "      ELSE 0 END\n",
        "    ) AS reported_risk\n",
        "  FROM `project-linkaja-dataset.pii.dim_merchant`\n",
        "  GROUP BY biz_org_id\n",
        "),\n",
        "\n",
        "CustomerRiskStatus AS (\n",
        "  SELECT\n",
        "    CAST(customer_id AS STRING) AS user_id,\n",
        "    SUM(\n",
        "      CASE WHEN status_desc = 'blocked' OR status_desc = 'frozen'\n",
        "      OR customer_id IN (SELECT user_id FROM `project-self-service-query.fraud_hotlist.reported_merchant_customer`) THEN 2\n",
        "      WHEN status_desc = 'suspended' THEN 1\n",
        "      ELSE 0 END\n",
        "    ) AS reported_risk\n",
        "  FROM `project-linkaja-dataset.pii.dim_customer`\n",
        "  GROUP BY customer_id\n",
        "),\n",
        "\n",
        "TransactionData AS (\n",
        "  SELECT\n",
        "    COALESCE(d.user_id, c.user_id) AS user_id,\n",
        "    COALESCE(d.outbound_trx, 0) AS outbound_trx,\n",
        "    COALESCE(c.inbound_trx, 0) AS inbound_trx\n",
        "  FROM\n",
        "    (\n",
        "        SELECT\n",
        "            CAST(debit_party_id AS STRING) AS user_id,\n",
        "            COUNT(orderid) AS outbound_trx\n",
        "        FROM `project-linkaja-dataset.pii.fact_transaction`\n",
        "        WHERE partition_initiate_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH)\n",
        "          AND debit_party_id IS NOT NULL\n",
        "          AND debit_party_id IN (SELECT CAST(debit_party_id AS INT64) FROM `project-self-service-query.fraud_hotlist.edges_fraudsampling` WHERE debit_party_id != 'ss_pertamina')\n",
        "        GROUP BY debit_party_id\n",
        "    ) d\n",
        "  FULL OUTER JOIN\n",
        "    (\n",
        "        SELECT\n",
        "            CAST(credit_party_id AS STRING) AS user_id,\n",
        "            COUNT(orderid) AS inbound_trx\n",
        "        FROM `project-linkaja-dataset.pii.fact_transaction`\n",
        "        WHERE partition_initiate_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH)\n",
        "          AND credit_party_id IS NOT NULL\n",
        "          AND credit_party_id IN (SELECT CAST(credit_party_id AS INT64) FROM `project-self-service-query.fraud_hotlist.edges_fraudsampling` WHERE credit_party_id != 'ss_pertamina')\n",
        "        GROUP BY credit_party_id\n",
        "    ) c ON d.user_id = c.user_id\n",
        "),\n",
        "\n",
        "AggregatedData AS (\n",
        "  SELECT * FROM (\n",
        "    SELECT\n",
        "      COALESCE(MD.user_id, CD.user_id, MHC.user_id, CHC.user_id, MRS.user_id, CRS.user_id) AS user_id,\n",
        "      COALESCE(MD.type, CD.type) AS type,\n",
        "      COALESCE(MD.province, CD.province) AS province,\n",
        "      COALESCE(MD.type_desc, CD.type_desc) AS type_desc,\n",
        "      COALESCE(MD.business_segment, CD.business_segment) AS business_segment,\n",
        "      COALESCE(MD.business_sub_segment, CD.business_sub_segment) AS business_sub_segment,\n",
        "      COALESCE(MD.account_age_months, CD.account_age_months) AS account_age_months,\n",
        "      TD.inbound_trx,\n",
        "      TD.outbound_trx,\n",
        "      COALESCE(MHC.hit_count, CHC.hit_count, 0) AS hit_count,\n",
        "      COALESCE(MRS.reported_risk, CRS.reported_risk, 0) AS reported_risk\n",
        "    FROM TransactionData TD\n",
        "    LEFT JOIN MerchantData MD ON TD.user_id = MD.user_id\n",
        "    LEFT JOIN CustomerData CD ON TD.user_id = CD.user_id\n",
        "    LEFT JOIN MerchantHitCounts MHC ON MD.user_id = MHC.user_id\n",
        "    LEFT JOIN CustomerHitCounts CHC ON CD.user_id = CHC.user_id\n",
        "    LEFT JOIN MerchantRiskStatus MRS ON MD.user_id = MRS.user_id\n",
        "    LEFT JOIN CustomerRiskStatus CRS ON CD.user_id = CRS.user_id\n",
        "    WHERE MD.user_id IS NOT NULL OR CD.user_id IS NOT NULL\n",
        "    # ORDER BY RAND()\n",
        "    # LIMIT 17000\n",
        "  )\n",
        "    UNION ALL\n",
        "  SELECT * FROM AggregatedData WHERE FALSE\n",
        "),\n",
        "\n",
        "CombinedNodes AS (\n",
        "  SELECT\n",
        "    CAST(user_id AS STRING) AS user_id,\n",
        "    'NULL' AS source_id,\n",
        "    'NULL' AS target_id,\n",
        "    NULL AS trans_amount,\n",
        "    'NULL' AS trans_initiate_time,\n",
        "    'node' AS data_type,\n",
        "    CAST(type AS STRING) AS type,\n",
        "    CAST(province AS STRING) AS province,\n",
        "    CAST(type_desc AS STRING) AS type_desc,\n",
        "    CAST(business_segment AS STRING) AS business_segment,\n",
        "    CAST(business_sub_segment AS STRING) AS business_sub_segment,\n",
        "    account_age_months,\n",
        "    outbound_trx,\n",
        "    inbound_trx,\n",
        "    hit_count,\n",
        "    reported_risk\n",
        "  FROM AggregatedData\n",
        "),\n",
        "\n",
        "CombinedEdges AS (\n",
        "  SELECT\n",
        "    'NULL' AS user_id,\n",
        "    CAST(debit_party_id AS STRING) AS source_id,\n",
        "    CAST(credit_party_id AS STRING) AS target_id,\n",
        "    trans_amount,\n",
        "    CAST(trans_initiate_time AS STRING) AS trans_initiate_time,\n",
        "    'edge' AS data_type,\n",
        "    'NULL' AS type,\n",
        "    'NULL' AS province,\n",
        "    'NULL' AS type_desc,\n",
        "    'NULL' AS business_segment,\n",
        "    'NULL' AS business_sub_segment,\n",
        "    NULL AS account_age_months,\n",
        "    NULL AS outbound_trx,\n",
        "    NULL AS inbound_trx,\n",
        "    NULL AS hit_count,\n",
        "    NULL AS reported_risk\n",
        "  FROM `project-linkaja-dataset.pii.fact_transaction`\n",
        "  WHERE partition_initiate_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH)\n",
        "  AND CAST(debit_party_id AS STRING) IN (SELECT user_id FROM AggregatedData WHERE user_id IS NOT NULL)\n",
        "  AND CAST(credit_party_id AS STRING) IN (SELECT user_id FROM AggregatedData WHERE user_id IS NOT NULL)\n",
        ")\n",
        "\n",
        "SELECT * FROM CombinedNodes\n",
        "UNION ALL\n",
        "SELECT * FROM CombinedEdges\n",
        "  \"\"\"\n",
        "\n",
        "  if runtime == \"colab\":\n",
        "    df = pd.read_gbq(query, project_id=project_id, dialect='standard')\n",
        "    # query_job = client.query(query)\n",
        "    # df = query_job.to_dataframe()\n",
        "  else:\n",
        "    client = bigquery.Client(project=project_id)\n",
        "    query_job = client.query(query)\n",
        "    df = query_job.to_dataframe()\n",
        "    \n",
        "  return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3NreJL4vFKts"
      },
      "outputs": [],
      "source": [
        "def preprocess(df):\n",
        "\t# Separating nodes and edges\n",
        "\tnode = df[['user_id', 'type', 'province', 'type_desc', 'business_segment', 'business_sub_segment', 'account_age_months', 'outbound_trx', 'inbound_trx', 'hit_count', 'reported_risk', 'data_type']]\n",
        "\tnodes = node[node['data_type'] == 'node'].drop('data_type', axis=1)\n",
        "\n",
        "\tedges = df[df['data_type'] == 'edge'].drop(['data_type'] + list(nodes.columns.difference(['source_id', 'target_id'])), axis=1)\n",
        "\n",
        "\tnodes['user_id'] = nodes['user_id'].astype('int64')\n",
        "\tnodes['account_age_months'] = nodes['account_age_months'].astype('int64')\n",
        "\tnodes['outbound_trx'] = nodes['outbound_trx'].astype('int64')\n",
        "\tnodes['inbound_trx'] = nodes['inbound_trx'].astype('int64')\n",
        "\tnodes['hit_count'] = nodes['hit_count'].astype('int64')\n",
        "\tnodes['reported_risk'] = nodes['reported_risk'].astype('int64')\n",
        "\n",
        "\tedges['source_id'] = edges['source_id'].astype('int64')\n",
        "\tedges['target_id'] = edges['target_id'].astype('int64')\n",
        "\tedges['trans_amount'] = edges['trans_amount'].astype('int64')\n",
        "\n",
        "\t#  Create a unified index mapping\n",
        "\tall_ids = pd.concat([nodes['user_id'], edges['source_id'], edges['target_id']]).unique()\n",
        "\tid_to_index = {id: idx for idx, id in enumerate(all_ids)}\n",
        "\n",
        "\t# Make a new column to replace original IDs with indices\n",
        "\tnodes['userid'] = nodes['user_id'].map(id_to_index)\n",
        "\tedges['sourceid'] = edges['source_id'].map(id_to_index)\n",
        "\tedges['targetid'] = edges['target_id'].map(id_to_index)\n",
        "\n",
        "\treturn nodes, edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DFKrsD6mlY9U"
      },
      "outputs": [],
      "source": [
        "### RGCN Model Training & Clustering ###\n",
        "\n",
        "class RGCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_relations):\n",
        "        super(RGCN, self).__init__()\n",
        "        self.conv1 = RGCNConv(in_channels, hidden_channels, num_relations=num_relations)\n",
        "        self.conv2 = RGCNConv(hidden_channels, out_channels, num_relations=num_relations)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_type):\n",
        "        x = F.relu(self.conv1(x, edge_index, edge_type=edge_type))\n",
        "        x = F.relu(self.conv2(x, edge_index, edge_type=edge_type))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "js8HeA8JEkem"
      },
      "outputs": [],
      "source": [
        "def encode(nodes, edges):\n",
        "    ### Preprocessing for RGCN ###\n",
        "\n",
        "    # Encode categorical attributes for nodes\n",
        "    encoder = LabelEncoder()\n",
        "    for col in ['type', 'province', 'type_desc', 'business_segment', 'business_sub_segment']:\n",
        "        nodes[col + '_code'] = encoder.fit_transform(nodes[col])\n",
        "    node_features = torch.tensor(nodes[['type_code', 'province_code', 'type_desc_code', 'business_segment_code', 'business_sub_segment_code', 'account_age_months', 'inbound_trx', 'outbound_trx']].values, dtype=torch.float)\n",
        "    # node_labels = torch.tensor(nodes['reported_risk'].values, dtype=torch.long)\n",
        "    # labeled_mask = node_labels != 0\n",
        "\n",
        "    edge_index = torch.tensor(edges[['sourceid', 'targetid']].values.T, dtype=torch.long)\n",
        "    edge_index[edge_index < 0] = 0\n",
        "    # edge_attr = torch.tensor(edges[['trans_amount']].values, dtype=torch.float)\n",
        "    edge_type = torch.zeros(edges.shape[0], dtype=torch.long)\n",
        "\n",
        "    return node_features, edge_index, edge_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_7edz2GcEken"
      },
      "outputs": [],
      "source": [
        "def load_model(model_path):\n",
        "\tmodel = RGCN(in_channels=8, hidden_channels=16, out_channels=3, num_relations=2)  # Adjust in_channels according to your input feature size\n",
        "\tmodel.load_state_dict(torch.load(model_path))\n",
        "\treturn model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FJ97E5FwEkeo"
      },
      "outputs": [],
      "source": [
        "def predict(model, node_features, edge_index, edge_type):\n",
        "\t###  Load Trained Model and Use It for Inference ###\n",
        "\tmodel.eval()\n",
        "\n",
        "\t# Generate embeddings\n",
        "\twith torch.no_grad():\n",
        "\t\tembeddings = model(node_features, edge_index, edge_type)\n",
        "\n",
        "\t# Use t-SNE to project embeddings to two dimensions for visualization\n",
        "\ttsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
        "\tembeddings_2d = tsne.fit_transform(embeddings)\n",
        "\n",
        "\t# Apply K-Means to the t-SNE 2D embeddings\n",
        "\tkmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
        "\tclusters = kmeans.fit_predict(embeddings_2d)\n",
        "\n",
        "\treturn clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YaqZurBJEker"
      },
      "outputs": [],
      "source": [
        "def make_graph(edge_index):\n",
        "    # Create a NetworkX graph from the edge_index tensor\n",
        "    G = nx.DiGraph()  # Use DiGraph to calculate in-degree and out-degree separately\n",
        "    edge_index_np = edge_index.cpu().numpy()\n",
        "    for i in range(edge_index_np.shape[1]):\n",
        "        source = edge_index_np[0, i].item()\n",
        "        target = edge_index_np[1, i].item()\n",
        "        G.add_edge(source, target)\n",
        "\n",
        "    # Calculate network metrics\n",
        "    degree_centrality = nx.degree_centrality(G)\n",
        "    in_degree_centrality = nx.in_degree_centrality(G)\n",
        "    out_degree_centrality = nx.out_degree_centrality(G)\n",
        "    betweenness_centrality = nx.betweenness_centrality(G)\n",
        "\n",
        "    # Convert centrality measures to DataFrame\n",
        "    centrality_df = pd.DataFrame({\n",
        "        'user_id': list(degree_centrality.keys()),\n",
        "        'degree_centrality': list(degree_centrality.values()),\n",
        "        'in_degree_centrality': list(in_degree_centrality.values()),\n",
        "        'out_degree_centrality': list(out_degree_centrality.values()),\n",
        "        'betweenness_centrality': list(betweenness_centrality.values())\n",
        "    })\n",
        "\n",
        "    return centrality_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uUrdRTbwEOYX"
      },
      "outputs": [],
      "source": [
        "def identify_syndicate(nodes, centrality_df):\n",
        "    # Normalize centrality measures to range between 0 and 1\n",
        "    centrality_df['normalized_in_degree'] = centrality_df['in_degree_centrality'] / centrality_df['in_degree_centrality'].max()\n",
        "    centrality_df['normalized_out_degree'] = centrality_df['out_degree_centrality'] / centrality_df['out_degree_centrality'].max()\n",
        "\n",
        "    # Define weights for each centrality measure\n",
        "    weights = {\n",
        "        'in_degree': 0.5,\n",
        "        'out_degree': 0.5\n",
        "    }\n",
        "\n",
        "    # Calculate the weighted syndicate score\n",
        "    centrality_df['syndicate_score'] = (weights['in_degree'] * centrality_df['normalized_in_degree'] +\n",
        "                                        weights['out_degree'] * centrality_df['normalized_out_degree'])\n",
        "\n",
        "    # Scale the score to be between 0 and 1\n",
        "    max_score = centrality_df['syndicate_score'].max()\n",
        "    if max_score > 0:  # Avoid division by zero\n",
        "        centrality_df['syndicate_score'] /= max_score\n",
        "\n",
        "    # Assuming 'nodes' is your main DataFrame containing user details and is globally accessible or passed as a parameter\n",
        "    nodes = pd.merge(nodes, centrality_df[['user_id', 'syndicate_score']], on='user_id', how='left')\n",
        "\n",
        "    return nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rCA_rLMXqbzr"
      },
      "outputs": [],
      "source": [
        "# Compliance risk mapping for multiple entries\n",
        "def map_compliance_risk(province, type_desc, business_segment, business_sub_segment):\n",
        "    # Define the compliance risk mapping\n",
        "    high_risk_values = (\n",
        "        'PT', 'Yayasan', 'Luar negeri', 'Jawa Timur', 'DKI Jakarta', 'Jawa Tengah', 'Jawa Barat',\n",
        "        'Non Profit', 'Corporate', 'e-commerce', 'Disbursement', 'Bank', 'E-Commerce/online',\n",
        "        'ECommerce/online', 'E-Commerce online', 'E-Commerceonline', 'ECommerceonline', 'Financial Service'\n",
        "    )\n",
        "    medium_risk_values = (\n",
        "        'CV', 'Koperasi', 'Perorangan', 'Jawa Barat', 'Jawa Timur', 'Sumatera Selatan',\n",
        "        'Sumatera Utara', 'Sumatera Selatan', 'Sumatera Utara', 'Aceh', 'Agent', 'retail', 'F&B',\n",
        "        'Utility', 'Entertainment', 'Internal and operational', 'Physical Retail', 'Transportation',\n",
        "        'Ticketing', 'Telco Product', 'Biller', 'Agent Non-Telco', 'Modern Channel', 'agentownshop',\n",
        "        'Modern Agent Telkomsel', 'Content', 'Games'\n",
        "    )\n",
        "\n",
        "    # Check if the input values belong to high or medium risk category\n",
        "    if province in high_risk_values or type_desc in high_risk_values or business_segment in high_risk_values or business_sub_segment in high_risk_values:\n",
        "        return 'high'\n",
        "    elif province in medium_risk_values or type_desc in medium_risk_values or business_segment in medium_risk_values or business_sub_segment in medium_risk_values:\n",
        "        return 'medium'\n",
        "    else:\n",
        "        return 'low'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ci0WQiayEkeq"
      },
      "outputs": [],
      "source": [
        "# Heuristic thresholds and mappings\n",
        "hit_count_thresholds = {'high': 332, 'medium': 3}\n",
        "\n",
        "# Function to calculate individual node risk scores\n",
        "def calculate_node_risk_score(row):\n",
        "    # Primary factor: Syndicate Score directly influences the base score\n",
        "    # Assuming syndicate_score is normalized between 0 and 1\n",
        "    base_score = row['syndicate_score']\n",
        "\n",
        "    # Secondary adjustments\n",
        "    # Hit count risk - smaller impact\n",
        "    if row['hit_count'] > hit_count_thresholds['high']:\n",
        "        base_score += 0.1  # Small adjustment for extreme cases\n",
        "    elif row['hit_count'] > hit_count_thresholds['medium']:\n",
        "        base_score += 0.05  # Even smaller adjustment for medium cases\n",
        "\n",
        "    # Compliance risk - using a mapped function to get risk adjustment\n",
        "    compliance_adjustment = map_compliance_risk(row['province'], row['type_desc'], row['business_segment'], row['business_sub_segment'])\n",
        "    if compliance_adjustment == 'high':\n",
        "        base_score += 0.1  # Compliance issues can adjust the score slightly\n",
        "    elif compliance_adjustment == 'medium':\n",
        "        base_score += 0.05\n",
        "\n",
        "    # Cap the score at 1.0 since the scale is 0 to 1\n",
        "    final_score = min(base_score, 1.0)\n",
        "    return final_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TrfXV-rFEkeq"
      },
      "outputs": [],
      "source": [
        "def calculate_risk_scores(nodes):\n",
        "\t# Calculate risk scores for each node\n",
        "\tnodes['risk_index'] = nodes.apply(calculate_node_risk_score, axis=1)\n",
        "\n",
        "\t# Aggregate risk scores by RGCN_cluster to get the average risk score for each cluster\n",
        "\tcluster_risk_scores = nodes.groupby('RGCN_cluster')['risk_index'].mean().reset_index()\n",
        "\tcluster_risk_scores.rename(columns={'risk_index': 'cluster_risk_score'}, inplace=True)\n",
        "\n",
        "\t# Calculate the thresholds for 'low' and 'high' risk levels based on quantiles\n",
        "\tlow_threshold = cluster_risk_scores['cluster_risk_score'].quantile(0.33)\n",
        "\thigh_threshold = cluster_risk_scores['cluster_risk_score'].quantile(0.67)\n",
        "\n",
        "\t# Define function to assign risk levels based on quantile thresholds\n",
        "\tdef assign_risk_level(final_score):\n",
        "\t\tif final_score >= high_threshold:\n",
        "\t\t\treturn 'high'\n",
        "\t\telif final_score > low_threshold:\n",
        "\t\t\treturn 'medium'\n",
        "\t\telse:\n",
        "\t\t\treturn 'low'\n",
        "\n",
        "\t# Assign risk levels based on quantiles\n",
        "\tcluster_risk_scores['cluster_risk_level'] = cluster_risk_scores['cluster_risk_score'].apply(assign_risk_level)\n",
        "\n",
        "\tnodes = nodes.merge(cluster_risk_scores, on='RGCN_cluster', how='left')\n",
        "\n",
        "\treturn nodes, cluster_risk_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Ae4nNVgESCjQ"
      },
      "outputs": [],
      "source": [
        "# Set adjustment factors\n",
        "degree_adjustment_factor = 0.05\n",
        "betweenness_adjustment_factor = 0.10\n",
        "def calculate_risk_index(cluster_risk_score, degree_centrality, betweenness_centrality):\n",
        "    # Adjust the risk index for each merchant node based on centrality measures\n",
        "    adjusted_risk_index = cluster_risk_score + \\\n",
        "        (degree_centrality * degree_adjustment_factor) + \\\n",
        "        (betweenness_centrality * betweenness_adjustment_factor)\n",
        "\n",
        "    # Normalize the adjusted risk index using Min-Max scaling\n",
        "    min_risk_index = adjusted_risk_index.min()\n",
        "    max_risk_index = adjusted_risk_index.max()\n",
        "    risk_index = (adjusted_risk_index - min_risk_index) / (max_risk_index - min_risk_index)\n",
        "\n",
        "    # Calculate quantile thresholds for 'low' and 'high' risk levels\n",
        "    low_quantile = risk_index.quantile(0.33)\n",
        "    high_quantile = risk_index.quantile(0.67)\n",
        "\n",
        "    # Define function to assign merchant risk levels based on quantile thresholds\n",
        "    def assign_risk_level(score):\n",
        "        if score >= high_quantile:\n",
        "            return 'high'\n",
        "        elif score > low_quantile:\n",
        "            return 'medium'\n",
        "        else:\n",
        "            return 'low'\n",
        "\n",
        "    # Assign risk levels based on quantiles\n",
        "    risk_level = risk_index.apply(assign_risk_level)\n",
        "\n",
        "    return adjusted_risk_index, risk_index, risk_level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7sYTWyD6QqyE"
      },
      "outputs": [],
      "source": [
        "def load_data_and_process(project_id):\n",
        "    # Assuming query is a function that retrieves your dataset\n",
        "    df = query(project_id)\n",
        "\n",
        "    # Assuming preprocess is a function that prepares your data\n",
        "    nodes, edges = preprocess(df)\n",
        "\n",
        "    # Assuming encode is a function that extracts features and encodes them for the model\n",
        "    node_features, edge_index, edge_type = encode(nodes, edges)\n",
        "\n",
        "    return nodes, edges, node_features, edge_index, edge_type\n",
        "\n",
        "def workflow(model_path, nodes, node_features, edge_index, edge_type):\n",
        "    # Load the predictive model\n",
        "    model = load_model(model_path)\n",
        "\n",
        "    # Predict cluster memberships\n",
        "    nodes['RGCN_cluster'] = predict(model, node_features, edge_index, edge_type)\n",
        "\n",
        "    # Generate graph and calculate centrality metrics\n",
        "    centrality_df = make_graph(edge_index)\n",
        "    nodes = nodes.merge(centrality_df, on='user_id', how='left')\n",
        "    nodes['degree_centrality'].fillna(0, inplace=True)\n",
        "    nodes['betweenness_centrality'].fillna(0, inplace=True)\n",
        "\n",
        "    # Identify syndicate nodes using the centrality data\n",
        "    nodes = identify_syndicate(nodes, centrality_df)\n",
        "\n",
        "    # Calculate risk scores incorporating new syndicate scores\n",
        "    nodes, cluster_risk_scores = calculate_risk_scores(nodes)\n",
        "\n",
        "    # Aggregate users within each cluster\n",
        "    clustered_users = nodes.groupby('RGCN_cluster')['user_id'].apply(list).reset_index()\n",
        "    clustered_users['user_id'] = clustered_users['user_id'].apply(lambda x: ', '.join(map(str, x)))\n",
        "\n",
        "    # Assuming calculate_risk_index is a function to adjust risk indices based on new metrics\n",
        "    adjusted_risk_index, risk_index, risk_level = calculate_risk_index(\n",
        "        nodes['cluster_risk_score'], nodes['degree_centrality'], nodes['betweenness_centrality'])\n",
        "\n",
        "    nodes['adjusted_risk_index'] = adjusted_risk_index\n",
        "    nodes['risk_index'] = risk_index\n",
        "    nodes['risk_level'] = risk_level\n",
        "\n",
        "    return nodes, cluster_risk_scores, clustered_users"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "kdenGVahW1ia"
      },
      "outputs": [],
      "source": [
        "def modify_nodes_df(nodes):\n",
        "    # Create new columns by aggregating other columns into dictionaries\n",
        "    nodes['attributes'] = nodes[['province', 'type_desc', 'business_segment',\n",
        "                                 'business_sub_segment', 'account_age_months', 'outbound_trx',\n",
        "                                 'inbound_trx', 'hit_count', 'reported_risk']].apply(lambda x: x.to_dict(), axis=1)\n",
        "    nodes['network_metrics'] = nodes[['degree_centrality',\n",
        "                                      'in_degree_centrality', 'out_degree_centrality',\n",
        "                                      'betweenness_centrality']].apply(lambda x: x.to_dict(), axis=1)\n",
        "\n",
        "    # Drop specified columns\n",
        "    nodes.drop(columns=['userid', 'type_code', 'province_code', 'type_desc_code',\n",
        "                        'business_segment_code', 'business_sub_segment_code'], inplace=True)\n",
        "\n",
        "    # Prepare final DataFrame\n",
        "    final_result = nodes[['user_id', 'type', 'attributes', 'network_metrics', 'syndicate_score', 'RGCN_cluster', 'cluster_risk_level', 'risk_level', 'cluster_risk_score', 'risk_index', 'adjusted_risk_index']]\n",
        "    final_result = final_result.sort_values(by='syndicate_score', ascending=False)\n",
        "\n",
        "    return final_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "KOBEVFcPQtVf",
        "outputId": "15bd2048-ec43-4c33-ce08-7b6e5891da3b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\KristyNatashaYohanes\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\auth\\_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
            "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>type</th>\n",
              "      <th>attributes</th>\n",
              "      <th>network_metrics</th>\n",
              "      <th>syndicate_score</th>\n",
              "      <th>RGCN_cluster</th>\n",
              "      <th>cluster_risk_level</th>\n",
              "      <th>risk_level</th>\n",
              "      <th>cluster_risk_score</th>\n",
              "      <th>risk_index</th>\n",
              "      <th>adjusted_risk_index</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>202000000091157829</td>\n",
              "      <td>customer</td>\n",
              "      <td>{'province': 'Jawa Tengah', 'type_desc': 'full...</td>\n",
              "      <td>{'degree_centrality': 0.0, 'in_degree_centrali...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>low</td>\n",
              "      <td>low</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>202000000087309664</td>\n",
              "      <td>customer</td>\n",
              "      <td>{'province': 'Jawa Tengah', 'type_desc': 'full...</td>\n",
              "      <td>{'degree_centrality': 0.0, 'in_degree_centrali...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>low</td>\n",
              "      <td>low</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>202000000095849332</td>\n",
              "      <td>customer</td>\n",
              "      <td>{'province': 'Jawa Tengah', 'type_desc': 'full...</td>\n",
              "      <td>{'degree_centrality': 0.0, 'in_degree_centrali...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>low</td>\n",
              "      <td>low</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>202100000009571443</td>\n",
              "      <td>customer</td>\n",
              "      <td>{'province': 'Jawa Timur', 'type_desc': 'full ...</td>\n",
              "      <td>{'degree_centrality': 0.0, 'in_degree_centrali...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>low</td>\n",
              "      <td>low</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>202100000007495194</td>\n",
              "      <td>customer</td>\n",
              "      <td>{'province': 'Jawa Barat', 'type_desc': 'full ...</td>\n",
              "      <td>{'degree_centrality': 0.0, 'in_degree_centrali...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>low</td>\n",
              "      <td>low</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1453</th>\n",
              "      <td>202000000039824612</td>\n",
              "      <td>customer</td>\n",
              "      <td>{'province': 'Jawa Timur', 'type_desc': 'full ...</td>\n",
              "      <td>{'degree_centrality': 0.0, 'in_degree_centrali...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>low</td>\n",
              "      <td>low</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1454</th>\n",
              "      <td>202000000097612804</td>\n",
              "      <td>customer</td>\n",
              "      <td>{'province': 'Jawa Barat', 'type_desc': 'full ...</td>\n",
              "      <td>{'degree_centrality': 0.0, 'in_degree_centrali...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>low</td>\n",
              "      <td>low</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1455</th>\n",
              "      <td>202000000062562373</td>\n",
              "      <td>customer</td>\n",
              "      <td>{'province': 'Jawa Barat', 'type_desc': 'full ...</td>\n",
              "      <td>{'degree_centrality': 0.0, 'in_degree_centrali...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>low</td>\n",
              "      <td>low</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1456</th>\n",
              "      <td>202100000009332538</td>\n",
              "      <td>customer</td>\n",
              "      <td>{'province': 'Jawa Barat', 'type_desc': 'full ...</td>\n",
              "      <td>{'degree_centrality': 0.0, 'in_degree_centrali...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>low</td>\n",
              "      <td>low</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1457</th>\n",
              "      <td>202100000007914149</td>\n",
              "      <td>customer</td>\n",
              "      <td>{'province': None, 'type_desc': 'basic service...</td>\n",
              "      <td>{'degree_centrality': 0.0, 'in_degree_centrali...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>low</td>\n",
              "      <td>low</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1458 rows Ã— 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 user_id      type  \\\n",
              "0     202000000091157829  customer   \n",
              "1     202000000087309664  customer   \n",
              "2     202000000095849332  customer   \n",
              "3     202100000009571443  customer   \n",
              "4     202100000007495194  customer   \n",
              "...                  ...       ...   \n",
              "1453  202000000039824612  customer   \n",
              "1454  202000000097612804  customer   \n",
              "1455  202000000062562373  customer   \n",
              "1456  202100000009332538  customer   \n",
              "1457  202100000007914149  customer   \n",
              "\n",
              "                                             attributes  \\\n",
              "0     {'province': 'Jawa Tengah', 'type_desc': 'full...   \n",
              "1     {'province': 'Jawa Tengah', 'type_desc': 'full...   \n",
              "2     {'province': 'Jawa Tengah', 'type_desc': 'full...   \n",
              "3     {'province': 'Jawa Timur', 'type_desc': 'full ...   \n",
              "4     {'province': 'Jawa Barat', 'type_desc': 'full ...   \n",
              "...                                                 ...   \n",
              "1453  {'province': 'Jawa Timur', 'type_desc': 'full ...   \n",
              "1454  {'province': 'Jawa Barat', 'type_desc': 'full ...   \n",
              "1455  {'province': 'Jawa Barat', 'type_desc': 'full ...   \n",
              "1456  {'province': 'Jawa Barat', 'type_desc': 'full ...   \n",
              "1457  {'province': None, 'type_desc': 'basic service...   \n",
              "\n",
              "                                        network_metrics  syndicate_score  \\\n",
              "0     {'degree_centrality': 0.0, 'in_degree_centrali...              NaN   \n",
              "1     {'degree_centrality': 0.0, 'in_degree_centrali...              NaN   \n",
              "2     {'degree_centrality': 0.0, 'in_degree_centrali...              NaN   \n",
              "3     {'degree_centrality': 0.0, 'in_degree_centrali...              NaN   \n",
              "4     {'degree_centrality': 0.0, 'in_degree_centrali...              NaN   \n",
              "...                                                 ...              ...   \n",
              "1453  {'degree_centrality': 0.0, 'in_degree_centrali...              NaN   \n",
              "1454  {'degree_centrality': 0.0, 'in_degree_centrali...              NaN   \n",
              "1455  {'degree_centrality': 0.0, 'in_degree_centrali...              NaN   \n",
              "1456  {'degree_centrality': 0.0, 'in_degree_centrali...              NaN   \n",
              "1457  {'degree_centrality': 0.0, 'in_degree_centrali...              NaN   \n",
              "\n",
              "      RGCN_cluster cluster_risk_level risk_level  cluster_risk_score  \\\n",
              "0                2                low        low                 NaN   \n",
              "1                1                low        low                 NaN   \n",
              "2                2                low        low                 NaN   \n",
              "3                2                low        low                 NaN   \n",
              "4                2                low        low                 NaN   \n",
              "...            ...                ...        ...                 ...   \n",
              "1453             1                low        low                 NaN   \n",
              "1454             1                low        low                 NaN   \n",
              "1455             1                low        low                 NaN   \n",
              "1456             0                low        low                 NaN   \n",
              "1457             3                low        low                 NaN   \n",
              "\n",
              "      risk_index  adjusted_risk_index  \n",
              "0            NaN                  NaN  \n",
              "1            NaN                  NaN  \n",
              "2            NaN                  NaN  \n",
              "3            NaN                  NaN  \n",
              "4            NaN                  NaN  \n",
              "...          ...                  ...  \n",
              "1453         NaN                  NaN  \n",
              "1454         NaN                  NaN  \n",
              "1455         NaN                  NaN  \n",
              "1456         NaN                  NaN  \n",
              "1457         NaN                  NaN  \n",
              "\n",
              "[1458 rows x 11 columns]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Main execution\n",
        "project_id = \"project-linkaja-dataset\"\n",
        "model_path = \"./models/RGCN_model.pth\"\n",
        "if runtime == \"colab\":\n",
        "    model_path = \"/content/drive/My Drive/RGCN_model.pth\"\n",
        "\n",
        "# Load data and prepare it for the model\n",
        "nodes, edges, node_features, edge_index, edge_type = load_data_and_process(project_id)\n",
        "\n",
        "# Run the model and calculate risk scores\n",
        "nodes, cluster_risk_scores, clustered_users = workflow(\n",
        "    model_path, nodes, node_features, edge_index, edge_type)\n",
        "\n",
        "final_result = modify_nodes_df(nodes)\n",
        "final_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "QZebNxE_cB79",
        "outputId": "7077afdd-a239-47b0-8a9b-1697bd9b067b"
      },
      "outputs": [],
      "source": [
        "numeric_stats = final_result.describe()\n",
        "numeric_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "hM4eDEQFWaoh",
        "outputId": "acdee144-80c0-4e32-c04c-f2267a2a2a8b"
      },
      "outputs": [],
      "source": [
        "cluster_risk_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "EZPre7EPWiL9",
        "outputId": "67164de3-ccbf-4cbb-8472-2b04a03a708a"
      },
      "outputs": [],
      "source": [
        "clustered_users"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
